{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ba2bcc",
   "metadata": {},
   "source": [
    "# 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "521e74b3-6d35-4988-8e1c-1743f20d3190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e8979ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T00:35:21.332505Z",
     "start_time": "2023-08-09T00:35:16.757995Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from annoy import AnnoyIndex\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f9a2d40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T00:35:21.379118Z",
     "start_time": "2023-08-09T00:35:21.365122Z"
    }
   },
   "outputs": [],
   "source": [
    "class PreTrainedEmbeddings(object):\n",
    "    \"\"\" 사전 훈련된 단어 벡터 사용을 위한 래퍼 클래스 \"\"\"\n",
    "\n",
    "    def __init__(self, word_to_index, word_vectors):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            word_to_index (dict): 단어에서 정수로 매핑\n",
    "            word_vectors (numpy 배열의 리스트)\n",
    "        \"\"\"\n",
    "        self.word_to_index = word_to_index\n",
    "        self.word_vectors = word_vectors\n",
    "        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\n",
    "\n",
    "        self.index = AnnoyIndex(len(word_vectors[0]), metric='euclidean')\n",
    "        print(\"인덱스 만드는 중!\")\n",
    "        for _, i in self.word_to_index.items():\n",
    "            self.index.add_item(i, self.word_vectors[i])\n",
    "        self.index.build(50)\n",
    "        print(\"완료!\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_embeddings_file(cls, embedding_file):\n",
    "        \"\"\"사전 훈련된 벡터 파일에서 객체를 만듭니다.\n",
    "\n",
    "        벡터 파일은 다음과 같은 포맷입니다:\n",
    "            word0 x0_0 x0_1 x0_2 x0_3 ... x0_N\n",
    "            word1 x1_0 x1_1 x1_2 x1_3 ... x1_N\n",
    "\n",
    "        매개변수:\n",
    "            embedding_file (str): 파일 위치\n",
    "        반환값:\n",
    "            PretrainedEmbeddings의 인스턴스\n",
    "        \"\"\"\n",
    "        word_to_index = {}\n",
    "        word_vectors = []\n",
    "\n",
    "        with open(embedding_file, encoding='utf-8') as fp:\n",
    "            for line in fp.readlines():\n",
    "                line = line.split(\" \")\n",
    "                word = line[0]\n",
    "                vec = np.array([float(x) for x in line[1:]])\n",
    "\n",
    "                word_to_index[word] = len(word_to_index)\n",
    "                word_vectors.append(vec)\n",
    "\n",
    "        return cls(word_to_index, word_vectors)\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            word (str)\n",
    "        반환값\n",
    "            임베딩 (numpy.ndarray)\n",
    "        \"\"\"\n",
    "        return self.word_vectors[self.word_to_index[word]]\n",
    "\n",
    "    def get_closest_to_vector(self, vector, n=1):\n",
    "        \"\"\"벡터가 주어지면 n 개의 최근접 이웃을 반환합니다\n",
    "        매개변수:\n",
    "            vector (np.ndarray): Annoy 인덱스에 있는 벡터의 크기와 같아야 합니다\n",
    "            n (int): 반환될 이웃의 개수\n",
    "        반환값:\n",
    "            [str, str, ...]: 주어진 벡터와 가장 가까운 단어\n",
    "                단어는 거리순으로 정렬되어 있지 않습니다.\n",
    "        \"\"\"\n",
    "        nn_indices = self.index.get_nns_by_vector(vector, n)\n",
    "        return [self.index_to_word[neighbor] for neighbor in nn_indices]\n",
    "\n",
    "    def compute_and_print_analogy(self, word1, word2, word3):\n",
    "        \"\"\"단어 임베딩을 사용한 유추 결과를 출력합니다\n",
    "\n",
    "        word1이 word2일 때 word3은 __입니다.\n",
    "        이 메서드는 word1 : word2 :: word3 : word4를 출력합니다\n",
    "\n",
    "        매개변수:\n",
    "            word1 (str)\n",
    "            word2 (str)\n",
    "            word3 (str)\n",
    "        \"\"\"\n",
    "        vec1 = self.get_embedding(word1)\n",
    "        vec2 = self.get_embedding(word2)\n",
    "        vec3 = self.get_embedding(word3)\n",
    "\n",
    "        # 네 번째 단어 임베딩을 계산합니다\n",
    "        spatial_relationship = vec2 - vec1\n",
    "        vec4 = vec3 + spatial_relationship\n",
    "\n",
    "        closest_words = self.get_closest_to_vector(vec4, n=4)\n",
    "        existing_words = set([word1, word2, word3])\n",
    "        closest_words = [word for word in closest_words\n",
    "                         if word not in existing_words]\n",
    "\n",
    "        if len(closest_words) == 0:\n",
    "            print(\"계산된 벡터와 가장 가까운 이웃을 찾을 수 없습니다!\")\n",
    "            return\n",
    "\n",
    "        for word4 in closest_words:\n",
    "            print(\"{} : {} :: {} : {}\".format(word1, word2, word3, word4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5843d283",
   "metadata": {},
   "source": [
    "## 단어 임베딩 사이의 관계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca5afec2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T00:51:08.206169Z",
     "start_time": "2023-08-09T00:51:08.174351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인덱스 만드는 중!\n",
      "완료!\n"
     ]
    }
   ],
   "source": [
    "embeddings = PreTrainedEmbeddings.from_embeddings_file(\n",
    "    '../data/05/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "476647ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T00:47:54.268576Z",
     "start_time": "2023-08-09T00:47:53.910857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : he :: woman : she\n",
      "man : he :: woman : her\n",
      "man : he :: woman : having\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy('man', 'he', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aac5bdd-8a98-4529-8eb3-ffa8b5fa4545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly : plane :: sail : ship\n",
      "fly : plane :: sail : vessel\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy('fly', 'plane', 'sail')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd0532-a216-4389-9fe9-5523cdb95711",
   "metadata": {},
   "source": [
    "# CBOW 임베딩 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78d8a850-807c-47b7-ab01-ff1c5a3ef2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b441386f-b9f7-4b92-965d-a2cfc94fef43",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e008ac83-7d92-4a54-9541-36308e31e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\" 매핑을 위해 텍스트를 처리하고 어휘 사전을 만드는 클래스 \"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, mask_token=\"<MASK>\", add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            token_to_idx (dict): 기존 토큰-인덱스 매핑 딕셔너리\n",
    "            mask_token (str): Vocabulary에 추가할 MASK 토큰.\n",
    "                모델 파라미터를 업데이트하는데 사용하지 않는 위치를 나타냅니다.\n",
    "            add_unk (bool): UNK 토큰을 추가할지 지정하는 플래그\n",
    "            unk_token (str): Vocabulary에 추가할 UNK 토큰\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token\n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self._mask_token = mask_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\" 직렬화할 수 있는 딕셔너리를 반환합니다 \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx,\n",
    "                'add_unk': self._add_unk,\n",
    "                'unk_token': self._unk_token,\n",
    "                'mask_token': self._mask_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" 직렬화된 딕셔너리에서 Vocabulary 객체를 만듭니다 \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\" 토큰을 기반으로 매핑 딕셔너리를 업데이트합니다\n",
    "\n",
    "        매개변수:\n",
    "            token (str): Vocabulary에 추가할 토큰\n",
    "        반환값:\n",
    "            index (int): 토큰에 상응하는 정수\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many(self, tokens):\n",
    "        \"\"\" 토큰 리스트를 Vocabulary에 추가합니다.\n",
    "\n",
    "        매개변수:\n",
    "            tokens (list): 문자열 토큰 리스트\n",
    "        반환값:\n",
    "            indices (list): 토큰 리스트에 상응되는 인덱스 리스트\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\" 토큰에 대응하는 인덱스를 추출합니다.\n",
    "        토큰이 없으면 UNK 인덱스를 반환합니다.\n",
    "\n",
    "        매개변수:\n",
    "            token (str): 찾을 토큰 \n",
    "        반환값:\n",
    "            index (int): 토큰에 해당하는 인덱스\n",
    "        노트:\n",
    "            UNK 토큰을 사용하려면 (Vocabulary에 추가하기 위해)\n",
    "            `unk_index`가 0보다 커야 합니다.\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\" 인덱스에 해당하는 토큰을 반환합니다.\n",
    "\n",
    "        매개변수: \n",
    "            index (int): 찾을 인덱스\n",
    "        반환값:\n",
    "            token (str): 인텍스에 해당하는 토큰\n",
    "        에러:\n",
    "            KeyError: 인덱스가 Vocabulary에 없을 때 발생합니다.\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa457c2e-172c-405f-bf2e-480f543c5700",
   "metadata": {},
   "source": [
    "## CBOWDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b849692-abce-4cc9-8ecc-261103fa028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, cbow_df, vectorizer):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            cbow_df (pandas.DataFrame): 데이터셋\n",
    "            vectorizer (CBOWVectorizer): 데이터셋에서 만든 CBOWVectorizer 객체\n",
    "        \"\"\"\n",
    "        self.cbow_df = cbow_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        def measure_len(context): return len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, cbow_df.context))\n",
    "\n",
    "        self.train_df = self.cbow_df[self.cbow_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.cbow_df[self.cbow_df.split == 'val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.cbow_df[self.cbow_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, cbow_csv):\n",
    "        \"\"\"데이터셋을 로드하고 처음부터 새로운 Vectorizer 만들기\n",
    "\n",
    "        매개변수:\n",
    "            cbow_csv (str): 데이터셋의 위치\n",
    "        반환값:\n",
    "            CBOWDataset의 인스턴스\n",
    "        \"\"\"\n",
    "        cbow_df = pd.read_csv(cbow_csv)\n",
    "        train_cbow_df = cbow_df[cbow_df.split == 'train']\n",
    "        return cls(cbow_df, CBOWVectorizer.from_dataframe(train_cbow_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, cbow_csv, vectorizer_filepath):\n",
    "        \"\"\" 데이터셋을 로드하고 새로운 CBOWVectorizer 객체를 만듭니다.\n",
    "        캐시된 CBOWVectorizer 객체를 재사용할 때 사용합니다.\n",
    "\n",
    "        매개변수:\n",
    "            cbow_csv (str): 데이터셋의 위치\n",
    "            vectorizer_filepath (str): CBOWVectorizer 객체의 저장 위치\n",
    "        반환값:\n",
    "            CBOWVectorizer의 인스턴스\n",
    "        \"\"\"\n",
    "        cbow_df = pd.read_csv(cbow_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(cbow_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"파일에서 CBOWVectorizer 객체를 로드하는 정적 메서드\n",
    "\n",
    "        매개변수:\n",
    "            vectorizer_filepath (str): 직렬화된 CBOWVectorizer 객체의 위치\n",
    "        반환값:\n",
    "            CBOWVectorizer의 인스턴스\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return CBOWVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"CBOWVectorizer 객체를 json 형태로 디스크에 저장합니다\n",
    "\n",
    "        매개변수:\n",
    "            vectorizer_filepath (str): CBOWVectorizer 객체의 저장 위치\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" 벡터 변환 객체를 반환합니다 \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" 데이터프레임에 있는 열을 사용해 분할 세트를 선택합니다 \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"파이토치 데이터셋의 주요 진입 메서드\n",
    "\n",
    "        매개변수:\n",
    "            index (int): 데이터 포인트의 인덱스\n",
    "        반환값:\n",
    "            데이터 포인트의 특성(x_data)과 레이블(y_target)로 이루어진 딕셔너리\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        context_vector = \\\n",
    "            self._vectorizer.vectorize(row.context, self._max_seq_length)\n",
    "        target_index = self._vectorizer.cbow_vocab.lookup_token(row.target)\n",
    "\n",
    "        return {'x_data': context_vector,\n",
    "                'y_target': target_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"배치 크기가 주어지면 데이터셋으로 만들 수 있는 배치 개수를 반환합니다\n",
    "\n",
    "        매개변수:\n",
    "            batch_size (int)\n",
    "        반환값:\n",
    "            배치 개수\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    파이토치 DataLoader를 감싸고 있는 제너레이터 함수.\n",
    "    걱 텐서를 지정된 장치로 이동합니다.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4160d65-2e78-451e-99b7-2ae94790a00b",
   "metadata": {},
   "source": [
    "## CBowVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1158c1e6-95ab-4fce-8852-b5feb6d5a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWVectorizer(object):\n",
    "    \"\"\" 어휘 사전을 생성하고 관리합니다 \"\"\"\n",
    "\n",
    "    def __init__(self, cbow_vocab):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            cbow_vocab (Vocabulary): 단어를 정수에 매핑합니다\n",
    "        \"\"\"\n",
    "        self.cbow_vocab = cbow_vocab\n",
    "\n",
    "    def vectorize(self, context, vector_length=-1):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            context (str): 공백으로 나누어진 단어 문자열\n",
    "            vector_length (int): 인덱스 벡터의 길이 매개변수\n",
    "        \"\"\"\n",
    "\n",
    "        indices = [self.cbow_vocab.lookup_token(\n",
    "            token) for token in context.split(' ')]\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.cbow_vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, cbow_df):\n",
    "        \"\"\"데이터셋 데이터프레임에서 Vectorizer 객체를 만듭니다\n",
    "\n",
    "        매개변수::\n",
    "            cbow_df (pandas.DataFrame): 타깃 데이터셋\n",
    "        반환값:\n",
    "            CBOWVectorizer 객체\n",
    "        \"\"\"\n",
    "        cbow_vocab = Vocabulary()\n",
    "        for index, row in cbow_df.iterrows():\n",
    "            for token in row.context.split(' '):\n",
    "                cbow_vocab.add_token(token)\n",
    "            cbow_vocab.add_token(row.target)\n",
    "\n",
    "        return cls(cbow_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        cbow_vocab = \\\n",
    "            Vocabulary.from_serializable(contents['cbow_vocab'])\n",
    "        return cls(cbow_vocab=cbow_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'cbow_vocab': self.cbow_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117beea7-b2be-46eb-ba2b-f03d044a425f",
   "metadata": {},
   "source": [
    "## CBowClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cefdbff2-f14e-421a-a0fb-fbc747618741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWClassifier(nn.Module):  # Simplified cbow Model\n",
    "    def __init__(self, vocabulary_size, embedding_size, padding_idx=0):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            vocabulary_size (int): 어휘 사전 크기, 임베딩 개수와 예측 벡터 크기를 결정합니다\n",
    "            embedding_size (int): 임베딩 크기\n",
    "            padding_idx (int): 기본값 0; 임베딩은 이 인덱스를 사용하지 않습니다\n",
    "        \"\"\"\n",
    "        super(CBOWClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size,\n",
    "                                      embedding_dim=embedding_size,\n",
    "                                      padding_idx=padding_idx)\n",
    "        self.fc1 = nn.Linear(in_features=embedding_size,\n",
    "                             out_features=vocabulary_size)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"분류기의 정방향 계산\n",
    "\n",
    "        매개변수:\n",
    "            x_in (torch.Tensor): 입력 데이터 텐서 \n",
    "                x_in.shape는 (batch, input_dim)입니다.\n",
    "            apply_softmax (bool): 소프트맥스 활성화 함수를 위한 플래그\n",
    "                크로스-엔트로피 손실을 사용하려면 False로 지정합니다\n",
    "        반환값:\n",
    "            결과 텐서. tensor.shape은 (batch, output_dim)입니다.\n",
    "        \"\"\"\n",
    "        x_embedded_sum = F.dropout(self.embedding(x_in).sum(dim=1), 0.3)\n",
    "        y_out = self.fc1(x_embedded_sum)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d0616-b37a-4deb-9dcb-93959583ca5b",
   "metadata": {},
   "source": [
    "## 모델훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f87fda1-bb71-4c33-a4ab-e1c4e653b298",
   "metadata": {},
   "source": [
    "### 헬퍼함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a554a9e-7f71-4dde-ae6d-b765ead68e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\" 훈련 상태를 업데이트합니다.\n",
    "\n",
    "    Components:\n",
    "     - 조기 종료: 과대 적합 방지\n",
    "     - 모델 체크포인트: 더 나은 모델을 저장합니다\n",
    "\n",
    "    :param args: 메인 매개변수\n",
    "    :param model: 훈련할 모델\n",
    "    :param train_state: 훈련 상태를 담은 딕셔너리\n",
    "    :returns:\n",
    "        새로운 훈련 상태\n",
    "    \"\"\"\n",
    "\n",
    "    # 적어도 한 번 모델을 저장합니다\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # 성능이 향상되면 모델을 저장합니다\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # 손실이 나빠지면\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # 조기 종료 단계 업데이트\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # 손실이 감소하면\n",
    "        else:\n",
    "            # 최상의 모델 저장\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # 조기 종료 단계 재설정\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # 조기 종료 여부 확인\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63848a01-87db-4bc2-88af-46ae087ec43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd2ac11-a31b-4be3-9c19-beb2018b55fe",
   "metadata": {},
   "source": [
    "### 설정과 전처리 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e35ca891-1d60-4c24-a17c-ff108c64d511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 경로: \n",
      "\t../model/05/cbow\\vectorizer.json\n",
      "\t../model/05/cbow\\model.pth\n",
      "CUDA 사용여부: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # 날짜와 경로 정보\n",
    "    cbow_csv=\"../data/05/books/frankenstein_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"../model/05/cbow\",\n",
    "    # 모델 하이퍼파라미터\n",
    "    embedding_size=50,\n",
    "    # 훈련 하이퍼파라미터\n",
    "    seed=1337,\n",
    "    num_epochs=100,\n",
    "    learning_rate=0.0001,\n",
    "    batch_size=32,\n",
    "    early_stopping_criteria=5,\n",
    "    # 실행 옵션\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "\n",
    "    print(\"파일 경로: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "\n",
    "# CUDA 체크\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "print(\"CUDA 사용여부: {}\".format(args.cuda))\n",
    "\n",
    "# 재현성을 위해 시드 설정\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# 디렉토리 처리\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bf24c66-4c6a-4201-bab9-1c453cb70bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋을 로드하고 Vectorizer를 만듭니다\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    print(\"데이터셋과 Vectorizer를 로드합니다\")\n",
    "    dataset = CBOWDataset.load_dataset_and_load_vectorizer(args.cbow_csv,\n",
    "                                                           args.vectorizer_file)\n",
    "else:\n",
    "    print(\"데이터셋을 로드하고 Vectorizer를 만듭니다\")\n",
    "    dataset = CBOWDataset.load_dataset_and_make_vectorizer(args.cbow_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = CBOWClassifier(vocabulary_size=len(vectorizer.cbow_vocab),\n",
    "                            embedding_size=args.embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a2a0d-45e7-4b9a-b36b-2bd9f7cc76b1",
   "metadata": {},
   "source": [
    "### 훈련반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3851e25d-b2eb-4805-988f-c566a310e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71ba487b-4fb5-4791-8653-584f9bcc7ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4669dec2f94520b4a57fe643700408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a141cbbf37ab4bad95fab58043463c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/1984 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735b0f9c45944c9f8e276d7e3a35a1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='training routine',\n",
    "                               total=args.num_epochs,\n",
    "                               position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm_notebook(desc='split=train',\n",
    "                               total=dataset.get_num_batches(args.batch_size),\n",
    "                               position=1,\n",
    "                               leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm_notebook(desc='split=val',\n",
    "                             total=dataset.get_num_batches(args.batch_size),\n",
    "                             position=1,\n",
    "                             leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # 훈련 세트에 대한 순회\n",
    "\n",
    "        # 훈련 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 훈련 과정은 5단계로 이루어집니다\n",
    "\n",
    "            # --------------------------------------\n",
    "            # 단계 1. 그레이디언트를 0으로 초기화합니다\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 단계 2. 출력을 계산합니다\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'])\n",
    "\n",
    "            # 단계 3. 손실을 계산합니다\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 4. 손실을 사용해 그레이디언트를 계산합니다\n",
    "            loss.backward()\n",
    "\n",
    "            # 단계 5. 옵티마이저로 가중치를 업데이트합니다\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "\n",
    "            # 정확도를 계산합니다\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # 진행 바 업데이트\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc,\n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # 검증 세트에 대한 순회\n",
    "\n",
    "        # 검증 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # 단계 1. 출력을 계산합니다\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'])\n",
    "\n",
    "            # 단계 2. 손실을 계산합니다\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 3. 정확도를 계산합니다\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc,\n",
    "                                epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28651b37-8c45-4e16-b6b9-b7e4bae65d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산합니다\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "classifier = classifier.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset,\n",
    "                                   batch_size=args.batch_size,\n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력을 계산합니다\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'])\n",
    "\n",
    "    # 손실을 계산합니다\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # 정확도를 계산합니다\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93ab20df-75c5-4902-8ea3-6a2aef95176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 7.672896479438331;\n",
      "테스트 정확도: 13.007352941176478\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {};\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84957c21-d04e-43c2-8074-30fedfb6b7cb",
   "metadata": {},
   "source": [
    "## 훈련된 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca0563ce-c513-4370-80f6-ccf179bdb275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(results):\n",
    "    \"\"\"\n",
    "    임베딩 결과를 출력합니다\n",
    "    \"\"\"\n",
    "    for item in results:\n",
    "        print(\"...[%.2f] - %s\" % (item[1], item[0]))\n",
    "\n",
    "\n",
    "def get_closest(target_word, word_to_idx, embeddings, n=5):\n",
    "    \"\"\"\n",
    "    n개의 최근접 단어를 찾습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # 다른 모든 단어까지 거리를 계산합니다\n",
    "    word_embedding = embeddings[word_to_idx[target_word.lower()]]\n",
    "    distances = []\n",
    "    for word, index in word_to_idx.items():\n",
    "        if word == \"<MASK>\" or word == target_word:\n",
    "            continue\n",
    "        distances.append((word, torch.dist(word_embedding, embeddings[index])))\n",
    "\n",
    "    results = sorted(distances, key=lambda x: x[1])[1:n+2]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "730297d7-b2d1-4840-9e0b-196594e6727c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "단어를 입력해 주세요:  monster\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...[7.57] - cares\n",
      "...[7.70] - griefs\n",
      "...[7.74] - saw\n",
      "...[7.78] - confused\n",
      "...[7.81] - without\n",
      "...[7.82] - truly\n"
     ]
    }
   ],
   "source": [
    "word = input('단어를 입력해 주세요: ')\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = vectorizer.cbow_vocab._token_to_idx\n",
    "pretty_print(get_closest(word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d2a5b22-2e1e-4fdf-a538-effb461c4f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======frankenstein=======\n",
      "...[7.24] - irradiated\n",
      "...[7.68] - enslaved\n",
      "...[7.71] - men\n",
      "...[7.75] - gush\n",
      "...[7.76] - mode\n",
      "...[7.76] - austria\n",
      "=======monster=======\n",
      "...[7.57] - cares\n",
      "...[7.70] - griefs\n",
      "...[7.74] - saw\n",
      "...[7.78] - confused\n",
      "...[7.81] - without\n",
      "...[7.82] - truly\n",
      "=======science=======\n",
      "...[7.02] - mutual\n",
      "...[7.02] - impression\n",
      "...[7.06] - mist\n",
      "...[7.16] - swelling\n",
      "...[7.24] - darkened\n",
      "...[7.30] - tempted\n",
      "=======sickness=======\n",
      "...[6.21] - while\n",
      "...[6.59] - awoke\n",
      "...[6.60] - foundations\n",
      "...[6.66] - consoles\n",
      "...[6.69] - literally\n",
      "...[6.69] - know\n",
      "=======lonely=======\n",
      "...[6.77] - excessive\n",
      "...[6.85] - moonlight\n",
      "...[6.90] - ought\n",
      "...[7.10] - bed\n",
      "...[7.12] - three\n",
      "...[7.20] - superhuman\n",
      "=======happy=======\n",
      "...[6.33] - bottom\n",
      "...[6.42] - penetrated\n",
      "...[6.44] - wand\n",
      "...[6.52] - chivalry\n",
      "...[6.52] - joys\n",
      "...[6.53] - altered\n"
     ]
    }
   ],
   "source": [
    "target_words = ['frankenstein', 'monster',\n",
    "                'science', 'sickness', 'lonely', 'happy']\n",
    "\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = vectorizer.cbow_vocab._token_to_idx\n",
    "\n",
    "for target_word in target_words:\n",
    "    print(f\"======={target_word}=======\")\n",
    "    if target_word not in word_to_idx:\n",
    "        print(\"Not in vocabulary\")\n",
    "        continue\n",
    "    pretty_print(get_closest(target_word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a61886-5635-437e-a3b4-eb559cd5f494",
   "metadata": {},
   "source": [
    "# AG 뉴스 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc9a36e7-feaf-4665-86f3-9666ad71fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1da55-a591-4317-a120-5444bf3c07e2",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f79e0f1-259a-4599-b805-b2e304fb0c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"매핑을 위해 텍스트를 처리하고 어휘 사전을 만드는 클래스 \"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            token_to_idx (dict): 기존 토큰-인덱스 매핑 딕셔너리\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token\n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\" 직렬화할 수 있는 딕셔너리를 반환합니다 \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" 직렬화된 딕셔너리에서 Vocabulary 객체를 만듭니다 \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\" 토큰을 기반으로 매핑 딕셔너리를 업데이트합니다\n",
    "\n",
    "        매개변수:\n",
    "            token (str): Vocabulary에 추가할 토큰\n",
    "        반환값:\n",
    "            index (int): 토큰에 상응하는 정수\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"토큰 리스트를 Vocabulary에 추가합니다.\n",
    "\n",
    "        매개변수:\n",
    "            tokens (list): 문자열 토큰 리스트\n",
    "        반환값:\n",
    "            indices (list): 토큰 리스트에 상응되는 인덱스 리스트\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"토큰에 대응하는 인덱스를 추출합니다.\n",
    "\n",
    "        매개변수:\n",
    "            token (str): 찾을 토큰 \n",
    "        반환값:\n",
    "            index (int): 토큰에 해당하는 인덱스\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\" 인덱스에 해당하는 토큰을 반환합니다.\n",
    "\n",
    "        매개변수: \n",
    "            index (int): 찾을 인덱스\n",
    "        반환값:\n",
    "            token (str): 인텍스에 해당하는 토큰\n",
    "        에러:\n",
    "            KeyError: 인덱스가 Vocabulary에 없을 때 발생합니다.\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f6b1da-c12e-4af1-8048-43a7475d4c54",
   "metadata": {},
   "source": [
    "### SequenceVocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9322d71-6047-4a13-9313-210883fdafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\" 토큰에 대응하는 인덱스를 추출합니다.\n",
    "        토큰이 없으면 UNK 인덱스를 반환합니다.\n",
    "\n",
    "        매개변수:\n",
    "            token (str): 찾을 토큰 \n",
    "        반환값:\n",
    "            index (int): 토큰에 해당하는 인덱스\n",
    "        노트:\n",
    "            UNK 토큰을 사용하려면 (Vocabulary에 추가하기 위해)\n",
    "            `unk_index`가 0보다 커야 합니다.\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71850513-4daa-45d7-a92d-cd4bf34bda77",
   "metadata": {},
   "source": [
    "## NewsVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e578449a-bb5b-4bb9-9744-ff415a646225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsVectorizer(object):\n",
    "    \"\"\" 어휘 사전을 생성하고 관리합니다 \"\"\"\n",
    "\n",
    "    def __init__(self, title_vocab, category_vocab):\n",
    "        self.title_vocab = title_vocab\n",
    "        self.category_vocab = category_vocab\n",
    "\n",
    "    def vectorize(self, title, vector_length=-1):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            title (str): 공백으로 나누어진 단어 문자열\n",
    "            vector_length (int): 인덱스 벡터의 길이 매개변수\n",
    "        반환값:\n",
    "            벡터로 변환된 제목 (numpy.array)\n",
    "        \"\"\"\n",
    "        indices = [self.title_vocab.begin_seq_index]\n",
    "        indices.extend(self.title_vocab.lookup_token(token)\n",
    "                       for token in title.split(\" \"))\n",
    "        indices.append(self.title_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.title_vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, news_df, cutoff=25):\n",
    "        \"\"\"데이터셋 데이터프레임에서 Vectorizer 객체를 만듭니다\n",
    "\n",
    "        매개변수:\n",
    "            news_df (pandas.DataFrame): 타깃 데이터셋\n",
    "            cutoff (int): Vocabulary에 포함할 빈도 임곗값\n",
    "        반환값:\n",
    "            NewsVectorizer 객체\n",
    "        \"\"\"\n",
    "        category_vocab = Vocabulary()\n",
    "        for category in sorted(set(news_df.category)):\n",
    "            category_vocab.add_token(category)\n",
    "\n",
    "        word_counts = Counter()\n",
    "        for title in news_df.title:\n",
    "            for token in title.split(\" \"):\n",
    "                if token not in string.punctuation:\n",
    "                    word_counts[token] += 1\n",
    "\n",
    "        title_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                title_vocab.add_token(word)\n",
    "\n",
    "        return cls(title_vocab, category_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        title_vocab = \\\n",
    "            SequenceVocabulary.from_serializable(contents['title_vocab'])\n",
    "        category_vocab =  \\\n",
    "            Vocabulary.from_serializable(contents['category_vocab'])\n",
    "\n",
    "        return cls(title_vocab=title_vocab, category_vocab=category_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'title_vocab': self.title_vocab.to_serializable(),\n",
    "                'category_vocab': self.category_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df40de-0412-4de6-8d65-b5f85ceb5a99",
   "metadata": {},
   "source": [
    "## NewsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3873f2e4-fda3-4d2b-8b6c-8c02e26ec0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, news_df, vectorizer):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            news_df (pandas.DataFrame): 데이터셋\n",
    "            vectorizer (NewsVectorizer): 데이터셋에서 만든 NewsVectorizer 객체\n",
    "        \"\"\"\n",
    "        self.news_df = news_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
    "        def measure_len(context): return len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, news_df.title)) + 2\n",
    "\n",
    "        self.train_df = self.news_df[self.news_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.news_df[self.news_df.split == 'val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.news_df[self.news_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "        # 클래스 가중치\n",
    "        class_counts = news_df.category.value_counts().to_dict()\n",
    "\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.category_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / \\\n",
    "            torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, news_csv):\n",
    "        \"\"\"데이터셋을 로드하고 처음부터 새로운 Vectorizer 만들기\n",
    "\n",
    "        매개변수:\n",
    "            news_csv (str): 데이터셋의 위치\n",
    "        반환값:\n",
    "            NewsDataset의 인스턴스\n",
    "        \"\"\"\n",
    "        news_df = pd.read_csv(news_csv)\n",
    "        train_news_df = news_df[news_df.split == 'train']\n",
    "        return cls(news_df, NewsVectorizer.from_dataframe(train_news_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, news_csv, vectorizer_filepath):\n",
    "        \"\"\" 데이터셋과 새로운 Vectorizer 객체를 로드합니다.\n",
    "        캐시된 Vectorizer 객체를 재사용할 때 사용합니다.\n",
    "\n",
    "        매개변수:\n",
    "            news_csv (str): 데이터셋의 위치\n",
    "            vectorizer_filepath (str): Vectorizer 객체의 저장 위치\n",
    "        반환값:\n",
    "            NewsDataset의 인스턴스\n",
    "        \"\"\"\n",
    "        news_df = pd.read_csv(news_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(news_csv, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"파일에서 Vectorizer 객체를 로드하는 정적 메서드\n",
    "\n",
    "        매개변수:\n",
    "            vectorizer_filepath (str): 직렬화된 Vectorizer 객체의 위치\n",
    "        반환값:\n",
    "            NewsVectorizer의 인스턴스\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"NewsVectorizer 객체를 json 형태로 디스크에 저장합니다\n",
    "\n",
    "        매개변수:\n",
    "            vectorizer_filepath (str): NewsVectorizer 객체의 저장 위치\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" 벡터 변환 객체를 반환합니다 \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" 데이터프레임에 있는 열을 사용해 분할 세트를 선택합니다 \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"파이토치 데이터셋의 주요 진입 메서드\n",
    "\n",
    "        매개변수:\n",
    "            index (int): 데이터 포인트의 인덱스\n",
    "        반환값:\n",
    "            데이터 포인트의 특성(x_data)과 레이블(y_target)로 이루어진 딕셔너리\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        title_vector = \\\n",
    "            self._vectorizer.vectorize(row.title, self._max_seq_length)\n",
    "\n",
    "        category_index = \\\n",
    "            self._vectorizer.category_vocab.lookup_token(row.category)\n",
    "\n",
    "        return {'x_data': title_vector,\n",
    "                'y_target': category_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"배치 크기가 주어지면 데이터셋으로 만들 수 있는 배치 개수를 반환합니다\n",
    "\n",
    "        매개변수:\n",
    "            batch_size (int)\n",
    "        반환값:\n",
    "            배치 개수\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    파이토치 DataLoader를 감싸고 있는 제너레이터 함수.\n",
    "    걱 텐서를 지정된 장치로 이동합니다.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42b59d-6df0-469b-aad9-de55a3b81970",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b30e1dd-89ca-4666-8b97-e4fc13d36eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, num_channels,\n",
    "                 hidden_dim, num_classes, dropout_p,\n",
    "                 pretrained_embeddings=None, padding_idx=0):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            embedding_size (int): 임베딩 벡터의 크기\n",
    "            num_embeddings (int): 임베딩 벡터의 개수\n",
    "            num_channels (int): 합성곱 커널 개수\n",
    "            hidden_dim (int): 은닉 차원 크기\n",
    "            num_classes (int): 클래스 개수\n",
    "            dropout_p (float): 드롭아웃 확률\n",
    "            pretrained_embeddings (numpy.array): 사전에 훈련된 단어 임베딩\n",
    "                기본값은 None \n",
    "            padding_idx (int): 패딩 인덱스\n",
    "        \"\"\"\n",
    "        super(NewsClassifier, self).__init__()\n",
    "\n",
    "        if pretrained_embeddings is None:\n",
    "\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
    "                                    num_embeddings=num_embeddings,\n",
    "                                    padding_idx=padding_idx)\n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(\n",
    "                pretrained_embeddings).float()\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
    "                                    num_embeddings=num_embeddings,\n",
    "                                    padding_idx=padding_idx,\n",
    "                                    _weight=pretrained_embeddings)\n",
    "\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_size,\n",
    "                      out_channels=num_channels, kernel_size=3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels,\n",
    "                      kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels,\n",
    "                      kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels,\n",
    "                      kernel_size=3),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self._dropout_p = dropout_p\n",
    "        self.fc1 = nn.Linear(num_channels, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"분류기의 정방향 계산\n",
    "\n",
    "        매개변수:\n",
    "            x_in (torch.Tensor): 입력 데이터 텐서 \n",
    "                x_in.shape는 (batch, dataset._max_seq_length)입니다.\n",
    "            apply_softmax (bool): 소프트맥스 활성화 함수를 위한 플래그\n",
    "                크로스-엔트로피 손실을 사용하려면 False로 지정합니다\n",
    "        반환값:\n",
    "            결과 텐서. tensor.shape은 (batch, num_classes)입니다.\n",
    "        \"\"\"\n",
    "\n",
    "        # 임베딩을 적용하고 특성과 채널 차원을 바꿉니다\n",
    "        x_embedded = self.emb(x_in).permute(0, 2, 1)\n",
    "\n",
    "        features = self.convnet(x_embedded)\n",
    "\n",
    "        # 평균 값을 계산하여 부가적인 차원을 제거합니다\n",
    "        remaining_size = features.size(dim=2)\n",
    "        features = F.avg_pool1d(features, remaining_size).squeeze(dim=2)\n",
    "        features = F.dropout(features, p=self._dropout_p)\n",
    "\n",
    "        # MLP 분류기\n",
    "        intermediate_vector = F.relu(\n",
    "            F.dropout(self.fc1(features), p=self._dropout_p))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989484a2-cd4b-4ddf-bb87-819853aba8d6",
   "metadata": {},
   "source": [
    "### 헬퍼함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2491016e-7e6a-4515-bf6f-aa1165958a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"훈련 상태를 업데이트합니다.\n",
    "\n",
    "    Components:\n",
    "     - 조기 종료: 과대 적합 방지\n",
    "     - 모델 체크포인트: 더 나은 모델을 저장합니다\n",
    "\n",
    "    :param args: 메인 매개변수\n",
    "    :param model: 훈련할 모델\n",
    "    :param train_state: 훈련 상태를 담은 딕셔너리\n",
    "    :returns:\n",
    "        새로운 훈련 상태\n",
    "    \"\"\"\n",
    "\n",
    "    # 적어도 한 번 모델을 저장합니다\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # 성능이 향상되면 모델을 저장합니다\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # 손실이 나빠지면\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # 조기 종료 단계 업데이트\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # 손실이 감소하면\n",
    "        else:\n",
    "            # 최상의 모델 저장\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # 조기 종료 단계 재설정\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # 조기 종료 여부 확인\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baf8531-2e35-41c7-954c-92824c79a123",
   "metadata": {},
   "source": [
    "### 유틸리티"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c540725c-2073-48d5-af58-0d0d217d434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "\n",
    "def load_glove_from_file(glove_filepath):\n",
    "    \"\"\"GloVe 임베딩 로드 \n",
    "\n",
    "    매개변수:\n",
    "        glove_filepath (str): 임베딩 파일 경로 \n",
    "    반환값:\n",
    "        word_to_index (dict), embeddings (numpy.ndarary)\n",
    "    \"\"\"\n",
    "\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(glove_filepath, \"rt\", encoding='utf-8') as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \")  # each line: word num1 num2 ...\n",
    "            word_to_index[line[0]] = index  # word = line[0]\n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    return word_to_index, np.stack(embeddings)\n",
    "\n",
    "\n",
    "def make_embedding_matrix(glove_filepath, words):\n",
    "    \"\"\"\n",
    "    특정 단어 집합에 대한 임베딩 행렬을 만듭니다.\n",
    "\n",
    "    매개변수:\n",
    "        glove_filepath (str): 임베딩 파일 경로\n",
    "        words (list): 단어 리스트\n",
    "    \"\"\"\n",
    "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
    "    embedding_size = glove_embeddings.shape[1]\n",
    "\n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1343763d-356d-4d9b-8541-36dc2b87def3",
   "metadata": {},
   "source": [
    "### 설정과 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46bf97a4-d88e-4efa-af81-456b8dfaa4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c71f1345-c1f9-491a-be41-da181045cb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 경로: \n",
      "\t../model/05/agnews\\vectorizer.json\n",
      "\t../model/05/agnews\\model.pth\n",
      "CUDA 사용여부: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # 날짜와 경로 정보\n",
    "    news_csv=\"../data/05/ag_news/news_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"../model/05/agnews\",\n",
    "    # 모델 하이퍼파라미터\n",
    "    glove_filepath='../data/05/glove.6B.100d.txt',\n",
    "    use_glove=False,\n",
    "    embedding_size=100,\n",
    "    hidden_dim=100,\n",
    "    num_channels=100,\n",
    "    # 훈련 하이퍼파라미터\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    dropout_p=0.1,\n",
    "    batch_size=128,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    # 실행 옵션\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "\n",
    "    print(\"파일 경로: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "# CUDA 체크\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"CUDA 사용여부: {}\".format(args.cuda))\n",
    "\n",
    "# 재현성을 위해 시드 설정\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# 디렉토리 처리\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87d7d2d3-a5e8-4e9d-bf71-558cc5f0d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.use_glove = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc2883f7-cfdc-497c-b108-18ad2eb75261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전 훈련된 임베딩을 사용합니다\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    # 체크포인트를 로드합니다.\n",
    "    dataset = NewsDataset.load_dataset_and_load_vectorizer(args.news_csv,\n",
    "                                                           args.vectorizer_file)\n",
    "else:\n",
    "    # 데이터셋과 Vectorizer를 만듭니다.\n",
    "    dataset = NewsDataset.load_dataset_and_make_vectorizer(args.news_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# GloVe를 사용하거나 랜덤하게 임베딩을 초기화합니다\n",
    "if args.use_glove:\n",
    "    words = vectorizer.title_vocab._token_to_idx.keys()\n",
    "    embeddings = make_embedding_matrix(glove_filepath=args.glove_filepath,\n",
    "                                       words=words)\n",
    "    print(\"사전 훈련된 임베딩을 사용합니다\")\n",
    "else:\n",
    "    print(\"사전 훈련된 임베딩을 사용하지 않습니다\")\n",
    "    embeddings = None\n",
    "\n",
    "classifier = NewsClassifier(embedding_size=args.embedding_size,\n",
    "                            num_embeddings=len(vectorizer.title_vocab),\n",
    "                            num_channels=args.num_channels,\n",
    "                            hidden_dim=args.hidden_dim,\n",
    "                            num_classes=len(vectorizer.category_vocab),\n",
    "                            dropout_p=args.dropout_p,\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09caf982-e521-4ff8-af7c-8e63a9bd757c",
   "metadata": {},
   "source": [
    "### 훈련반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "debce048-7454-4504-a23a-93ab2d2aae2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97428dabe2046859f64f19445194bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f65ef83c0e42b7b229a6acb9e00b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09c8cbe12824b8fac770925efc66a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='training routine',\n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm_notebook(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size),\n",
    "                          position=1,\n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm_notebook(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size),\n",
    "                        position=1,\n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # 훈련 세트에 대한 순회\n",
    "\n",
    "        # 훈련 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 훈련 과정은 5단계로 이루어집니다\n",
    "\n",
    "            # --------------------------------------\n",
    "            # 단계 1. 그레이디언트를 0으로 초기화합니다\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 단계 2. 출력을 계산합니다\n",
    "            y_pred = classifier(batch_dict['x_data'])\n",
    "\n",
    "            # 단계 3. 손실을 계산합니다\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 4. 손실을 사용해 그레이디언트를 계산합니다\n",
    "            loss.backward()\n",
    "\n",
    "            # 단계 5. 옵티마이저로 가중치를 업데이트합니다\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "\n",
    "            # 정확도를 계산합니다\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # 진행 상태 막대 업데이트\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc,\n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # 검증 세트에 대한 순회\n",
    "\n",
    "        # 검증 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # 단계 1. 출력을 계산합니다\n",
    "            y_pred = classifier(batch_dict['x_data'])\n",
    "\n",
    "            # 단계 2. 손실을 계산합니다\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 3. 정확도를 계산합니다\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc,\n",
    "                                epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "96f3792a-c88f-4e54-b2d3-cffe613aee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산합니다\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset,\n",
    "                                   batch_size=args.batch_size,\n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력을 계산합니다\n",
    "    y_pred = classifier(batch_dict['x_data'])\n",
    "\n",
    "    # 손실을 계산합니다\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # 정확도를 계산합니다\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf78e8d4-0496-4d22-9cb0-0e2287efc1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 0.7748141420739035;\n",
      "테스트 정확도: 78.8783482142857\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {};\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b96111-7b5c-4938-9548-610b348b3288",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b3c2afee-7d61-406e-98e8-117c04a141e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰 텍스트를 전처리합니다\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b787687-f26d-4247-a45a-5f214ce144f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(title, classifier, vectorizer, max_length):\n",
    "    \"\"\"뉴스 제목을 기반으로 카테고리를 예측합니다\n",
    "\n",
    "    매개변수:\n",
    "        title (str): 원시 제목 문자열\n",
    "        classifier (NewsClassifier): 훈련된 분류기 객체\n",
    "        vectorizer (NewsVectorizer): 해당 Vectorizer\n",
    "        max_length (int): 최대 시퀀스 길이\n",
    "            노트: CNN은 입력 텐서 크기에 민감합니다. \n",
    "                 훈련 데이터처럼 동일한 크기를 갖도록 만듭니다.\n",
    "    \"\"\"\n",
    "    title = preprocess_text(title)\n",
    "    vectorized_title = \\\n",
    "        torch.tensor(vectorizer.vectorize(title, vector_length=max_length))\n",
    "    result = classifier(vectorized_title.unsqueeze(0), apply_softmax=True)\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    predicted_category = vectorizer.category_vocab.lookup_index(indices.item())\n",
    "\n",
    "    return {'category': predicted_category,\n",
    "            'probability': probability_values.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5381bf3-9486-46c4-a222-b25295a3aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples():\n",
    "    samples = {}\n",
    "    for cat in dataset.val_df.category.unique():\n",
    "        samples[cat] = dataset.val_df.title[dataset.val_df.category == cat].tolist()[\n",
    "            :5]\n",
    "    return samples\n",
    "\n",
    "\n",
    "val_samples = get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e480c54b-c1f5-4c85-8b96-a38555bd99e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Category: Business\n",
      "==============================\n",
      "예측: Business (p=0.78)\n",
      "\t + 샘플: AZ suspends marketing of cancer drug\n",
      "예측: Business (p=1.00)\n",
      "\t + 샘플: Business world has mixed reaction to Perez move\n",
      "예측: Sports (p=0.85)\n",
      "\t + 샘플: Betting Against Bombay\n",
      "예측: Sci/Tech (p=0.38)\n",
      "\t + 샘플: Malpractice Insurers Face a Tough Market\n",
      "예측: Sports (p=0.65)\n",
      "\t + 샘플: NVIDIA Is Vindicated\n",
      "------------------------------\n",
      "\n",
      "True Category: Sci/Tech\n",
      "==============================\n",
      "예측: World (p=0.45)\n",
      "\t + 샘플: Spies prize webcam #39;s eyes\n",
      "예측: Sci/Tech (p=1.00)\n",
      "\t + 샘플: Sober worm causes headaches\n",
      "예측: World (p=0.98)\n",
      "\t + 샘플: Local Search: Missing Pieces Falling into Place\n",
      "예측: Sci/Tech (p=1.00)\n",
      "\t + 샘플: Hackers baiting Internet users with Beckham pix\n",
      "예측: Sports (p=1.00)\n",
      "\t + 샘플: Nokia adds BlackBerry support to Series 80 handsets\n",
      "------------------------------\n",
      "\n",
      "True Category: Sports\n",
      "==============================\n",
      "예측: Sci/Tech (p=0.70)\n",
      "\t + 샘플: Is Meyer the man to get Irish up?\n",
      "예측: Sports (p=0.36)\n",
      "\t + 샘플: Who? Who? And Clemens\n",
      "예측: Sports (p=1.00)\n",
      "\t + 샘플: Baseball Today (AP)\n",
      "예측: Sports (p=0.68)\n",
      "\t + 샘플: Mark Kreidler: Yao Ming epitomizes the Chinese athlete who is &lt;b&gt;...&lt;/b&gt;\n",
      "예측: Sports (p=0.97)\n",
      "\t + 샘플: No. 5 Miami Rebounds to Beat FSU in Overtime\n",
      "------------------------------\n",
      "\n",
      "True Category: World\n",
      "==============================\n",
      "예측: Sci/Tech (p=0.94)\n",
      "\t + 샘플: Arafat in pain but expected to recover-Shaath\n",
      "예측: World (p=1.00)\n",
      "\t + 샘플: Maoist rebels bomb Kathmandu building, no injuries (Reuters)\n",
      "예측: World (p=0.99)\n",
      "\t + 샘플: Son Running for Ill. Rep.'s House Seat (AP)\n",
      "예측: World (p=0.61)\n",
      "\t + 샘플: Strong Quake Hits in Japan\n",
      "예측: World (p=1.00)\n",
      "\t + 샘플: Israel assassinates Hamas militant in Damascus\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#title = input(\"Enter a news title to classify: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "\n",
    "for truth, sample_group in val_samples.items():\n",
    "    print(f\"True Category: {truth}\")\n",
    "    print(\"=\"*30)\n",
    "    for sample in sample_group:\n",
    "        prediction = predict_category(sample, classifier,\n",
    "                                      vectorizer, dataset._max_seq_length + 1)\n",
    "        print(\"예측: {} (p={:0.2f})\".format(prediction['category'],\n",
    "                                          prediction['probability']))\n",
    "        print(\"\\t + 샘플: {}\".format(sample))\n",
    "    print(\"-\"*30 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
