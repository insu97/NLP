{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e155bc89",
   "metadata": {},
   "source": [
    "# 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad8428e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:00:37.065373Z",
     "start_time": "2023-12-27T05:00:35.318953Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0fc8adc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:00:37.081425Z",
     "start_time": "2023-12-27T05:00:37.071413Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd39bac8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:00:37.345778Z",
     "start_time": "2023-12-27T05:00:37.086424Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# punkt : 마침표나 약어와 같은 특별한 언어적 특성을 고려하여 토큰화를 할 수 있게 해주는 모듈\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77837b10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:00:37.361671Z",
     "start_time": "2023-12-27T05:00:37.348760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Although', 'it', \"'s\", 'not', 'a', 'happily-ever-after', 'ending', ',', 'it', 'is', 'very', 'realistic', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Although it's not a happily-ever-after ending, it is very realistic.\"\n",
    "\n",
    "# 단어 토큰화\n",
    "tokenized_words = word_tokenize(text)\n",
    "\n",
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b6a1fa",
   "metadata": {},
   "source": [
    "# 정제(Cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d7e6970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:00:37.376679Z",
     "start_time": "2023-12-27T05:00:37.363672Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(''))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d33c813",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:00:37.391781Z",
     "start_time": "2023-12-27T05:00:37.379713Z"
    }
   },
   "outputs": [],
   "source": [
    "from data import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1914357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:00:37.407766Z",
     "start_time": "2023-12-27T05:00:37.395774Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = text.TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b81df56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:00:37.422924Z",
     "start_time": "2023-12-27T05:00:37.411354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cfe6b13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:00:37.438290Z",
     "start_time": "2023-12-27T05:00:37.425818Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42c5f3fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:00:37.469758Z",
     "start_time": "2023-12-27T05:00:37.443292Z"
    }
   },
   "outputs": [],
   "source": [
    "# 전체 단어 토큰 리스트\n",
    "tokenized_words = word_tokenize(corpus)\n",
    "\n",
    "# 파이썬의 Counter 모듈을 통해 단어의 빈도수 카운트하여 단어 집합 생성\n",
    "vocab = Counter(tokenized_words)\n",
    "\n",
    "# 빈도수가 2 이하인 단어 리스트 추출\n",
    "uncommon_words = [key for key, value in vocab.items() if value <= 2]\n",
    "\n",
    "# 빈도수가 2 이하인 단어들만 제거한 결과를 따로 저장\n",
    "cleaned_by_freq = [word for word in tokenized_words if word not in uncommon_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff9997a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:00:37.485130Z",
     "start_time": "2023-12-27T05:00:37.473313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 30, '.': 28, ',': 21, 'of': 15, 'and': 14, 'to': 13, 'a': 12, 'military': 12, 'in': 12, 'people': 9, 'on': 9, 'are': 9, 'for': 7, 'this': 7, 'that': 6, 'I': 5, 'The': 5, 'you': 5, 'not': 4, 'or': 4, 'about': 4, 'US': 4, 'at': 4, 'every': 4, 'it': 4, 'make': 4, 'was': 4, 'movie': 3, 'be': 3, 'who': 3, 'they': 3, 'Abu-Gharib': 3, 'makes': 3, 'number': 3, 'million': 3, 'with': 3, 'total': 3, 'would': 3, 'an': 3, 'there': 3, 'days': 3, 'hour': 3, 'minimum': 3, 'get': 3, 'comments': 2, ')': 2, 'know': 2, 'nothing': 2, 'base': 2, 'state': 2, 'world': 2, 'time': 2, ':': 2, '2.3': 2, 'indicted': 2, 'than': 2, 'That': 2, \"'s\": 2, 'but': 2, 'reality': 2, 'is': 2, 'first': 2, 'aid': 2, 'When': 2, 'their': 2, 'Within': 2, 'hours': 2, 'food': 2, 'months': 2, 'But': 2, 'website': 2, 'men': 2, 'women': 2, 'so': 2, 'personal': 2, 'gain': 2, 'under': 2, '40': 2, 'work': 2, 'week': 2, 'much': 2, 'ranks': 2, 'degrees': 2, 'After': 1, 'reading': 1, 'am': 1, 'sure': 1, 'whether': 1, 'should': 1, 'angry': 1, 'sad': 1, 'sickened': 1, 'Seeing': 1, 'typical': 1, 'absolutely': 1, 'b': 1, 'everything': 1, 'think': 1, 'movies': 1, 'like': 1, 'CNN': 1, 'reports': 1, 'me': 1, 'wonder': 1, 'intellectual': 1, 'stimulation': 1, 'At': 1, 'type': 1, '1.4': 1, 'Active': 1, 'Duty': 1, 'another': 1, 'almost': 1, '900,000': 1, 'Guard': 1, 'Reserves': 1, 'roughly': 1, 'abuses': 1, 'Currently': 1, 'less': 1, '20': 1, '.00083': 1, '%': 1, 'Even': 1, 'if': 1, 'indict': 1, 'single': 1, 'member': 1, 'ever': 1, 'stepped': 1, 'come': 1, 'close': 1, 'making': 1, 'whole': 1, 'flaws': 1, 'take': 1, 'YEARS': 1, 'cover': 1, 'understand': 1, 'supposed': 1, 'sarcastic': 1, 'writer': 1, 'director': 1, 'trying': 1, 'commentary': 1, 'without': 1, 'enemy': 1, 'fight': 1, 'In': 1, 'has': 1, 'been': 1, 'its': 1, 'busiest': 1, 'when': 1, 'conflicts': 1, 'going': 1, 'called': 1, 'disaster': 1, 'relief': 1, 'humanitarian': 1, 'missions': 1, 'tsunami': 1, 'hit': 1, 'Indonesia': 1, 'devestating': 1, 'region': 1, 'scene': 1, 'chaos': 1, 'situation': 1, 'overwhelmed': 1, 'local': 1, 'governments': 1, 'leadership': 1, 'looked': 1, 'same': 1, 'mocks': 1, 'said': 1, 'happen': 1, 'reaching': 1, 'isolated': 1, 'villages': 1, 'airfields': 1, 'were': 1, 'built': 1, 'cargo': 1, 'aircraft': 1, 'started': 1, 'landing': 1, 'distribution': 1, 'system': 1, 'up': 1, 'running': 1, 'Hours': 1, 'weeks': 1, 'Yes': 1, 'unscrupulous': 1, 'then': 1, 'walk': 1, 'life': 1, 'occupation': 1, 'see': 1, 'decide': 1, 'all': 1, 'criminal': 1, 'minds': 1, 'thoughts': 1, 'destruction': 1, 'mayhem': 1, 'absolute': 1, 'disservice': 1, 'things': 1, 'do': 1, 'day': 1, 'One': 1, 'person': 1, 'even': 1, 'went': 1, 'far': 1, 'as': 1, 'say': 1, 'members': 1, 'Wow': 1, '!': 1, 'Entry': 1, 'level': 1, 'personnel': 1, 'just': 1, '$': 1, '8.00': 1, 'assuming': 1, 'Of': 1, 'course': 1, 'many': 1, 'more': 1, 'those': 1, 'harm': 1, 'way': 1, 'typically': 1, 'put': 1, '16-18': 1, 'end': 1, 'pay': 1, 'well': 1, 'wage': 1, 'So': 1, 'beg': 1, 'please': 1, 'yourself': 1, 'familiar': 1, 'around': 1, 'Go': 1, 'nearby': 1, 'visitor': 1, 'pass': 1, 'meet': 1, 'some': 1, 'quick': 1, 'disparage': 1, 'You': 1, 'surprised': 1, 'no': 1, 'longer': 1, 'accepts': 1, 'lieu': 1, 'prison': 1, 'They': 1, 'require': 1, 'GED': 1, 'prefer': 1, 'high': 1, 'school': 1, 'diploma': 1, 'middle': 1, 'expected': 1, 'undergraduate': 1, 'upper': 1, 'encouraged': 1, 'advanced': 1})\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06646ea7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:00:37.501148Z",
     "start_time": "2023-12-27T05:00:37.487653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈도수가 2 이하인 단어 수: 234\n"
     ]
    }
   ],
   "source": [
    "print('빈도수가 2 이하인 단어 수:', len(uncommon_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "080db4fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:00:37.516448Z",
     "start_time": "2023-12-27T05:00:37.504151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈도수 3 이상인 토큰 수: 306\n"
     ]
    }
   ],
   "source": [
    "print('빈도수 3 이상인 토큰 수:', len(cleaned_by_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c639899",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:00:37.531862Z",
     "start_time": "2023-12-27T05:00:37.519445Z"
    }
   },
   "outputs": [],
   "source": [
    "# 길이가 2 이하인 단어 제거\n",
    "cleaned_by_freq_len = []\n",
    "\n",
    "for word in cleaned_by_freq:\n",
    "    if len(word) > 2:\n",
    "        cleaned_by_freq_len.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40fecc90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:00:37.547052Z",
     "start_time": "2023-12-27T05:00:37.534863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정제 전: ['the', 'for', 'this', 'movie', ',', 'I', 'not', 'I', 'be', ',']\n",
      "정제 후: ['the', 'for', 'this', 'movie', 'not', 'people', 'who', 'about', 'the', 'military']\n"
     ]
    }
   ],
   "source": [
    "# 정제 결과 확인\n",
    "print('정제 전:', cleaned_by_freq[:10])\n",
    "print('정제 후:', cleaned_by_freq_len[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb58b1",
   "metadata": {},
   "source": [
    "## 정제 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24265023",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:05:04.615064Z",
     "start_time": "2023-12-27T05:05:04.600938Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 등장 빈도 기준 정제 함수\n",
    "def clean_by_freq(tokenized_words, cut_off_count):\n",
    "    # 파이썬의 Counter 모듈을 통해 단어의 빈도수 카운트하여 단어 집합 생성\n",
    "    vocab = Counter(tokenized_words)\n",
    "    \n",
    "    # 빈도수가 cut_off_count 이하인 단어 set 추출\n",
    "    uncommon_words = {key for key, value in vocab.items() if value <= cut_off_count}\n",
    "    \n",
    "    # uncommon_words에 포함되지 않는 단어 리스트 생성\n",
    "    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n",
    "\n",
    "    return cleaned_words\n",
    "\n",
    "# 단어 길이 기준 정제 함수\n",
    "def clean_by_len(tokenized_words, cut_off_length):\n",
    "    # 길이가 cut_off_length 이하인 단어 제거\n",
    "    cleaned_by_freq_len = []\n",
    "    \n",
    "    for word in tokenized_words:\n",
    "        if len(word) > cut_off_length:\n",
    "            cleaned_by_freq_len.append(word)\n",
    "\n",
    "    return cleaned_by_freq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4d0e9fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:05:04.631044Z",
     "start_time": "2023-12-27T05:05:04.618046Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_by_freq = clean_by_freq(tokenized_words, 2)\n",
    "cleaned_words = clean_by_len(clean_by_freq, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9150564d",
   "metadata": {},
   "source": [
    "## 불용어(stopwords)\n",
    "    - Corpus에서 큰 의미가 없거나, 분석 목적에 벗어나는 단어들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cbc9fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:05:04.651886Z",
     "start_time": "2023-12-27T05:05:04.634211Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a46d0c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:05:04.678001Z",
     "start_time": "2023-12-27T05:05:04.657406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "{'he', 'above', 'then', 'which', 'those', 'she', 'their', 'until', \"weren't\", 'her', 'not', 'them', 'or', 'again', 'very', 'hasn', 'who', 'didn', 'mustn', 're', 'both', 'and', 'before', 'aren', \"wouldn't\", 'ours', 'being', 'at', 'having', \"needn't\", \"hasn't\", \"isn't\", 'wouldn', 'me', 'most', 'ma', 'mightn', 'while', 'your', 'too', 'other', 'themselves', 'than', 'was', 'because', \"shan't\", 'am', 'from', 'after', 'how', 'if', 'had', \"it's\", 'now', 'won', 'a', 'do', 'isn', 'why', 'these', 'during', 'between', 'on', 'only', 'of', \"you're\", 'haven', 'his', 'for', 'off', 'ain', 'be', 'such', \"you'd\", 'shan', 'what', 'm', \"mustn't\", 'no', 'there', 'that', 'all', 'd', 'just', 'its', 'over', 'in', 'needn', 'hadn', 'up', 'it', 'has', 'an', 've', \"hadn't\", 'ourselves', 'does', 'couldn', 'same', \"doesn't\", 'my', 'some', \"won't\", 'through', \"shouldn't\", 'is', 'don', 'yours', 'weren', 'were', 'below', 'down', 'herself', 'been', 'nor', 'but', 'myself', 'should', 'shouldn', 'to', 'once', 'have', 'with', 'whom', \"she's\", 'out', 'each', 'about', 'did', \"you'll\", 'can', 'o', \"wasn't\", \"mightn't\", 'they', 'wasn', \"couldn't\", 'doing', 'himself', 'theirs', 'any', 't', 'into', 'doesn', 'by', \"you've\", 'this', 'are', \"don't\", 'here', 'further', 'we', 's', 'itself', 'the', 'y', 'our', 'when', \"didn't\", 'under', 'where', 'as', 'more', 'yourselves', 'own', 'you', 'few', 'so', 'i', 'against', \"aren't\", \"should've\", 'yourself', 'hers', 'will', 'him', 'll', \"that'll\", \"haven't\"}\n"
     ]
    }
   ],
   "source": [
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "print('불용어 개수 :', len(stopwords_set))\n",
    "print(stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9de0585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:05:36.377654Z",
     "start_time": "2023-12-27T05:05:36.362481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 178\n",
      "불용어 출력 : {'he', 'above', 'then', 'which', 'those', 'she', 'their', 'until', \"weren't\", 'her', 'not', 'them', 'or', 'again', 'very', 'hasn', 'who', 'didn', 'mustn', 're', 'both', 'and', 'before', 'aren', \"wouldn't\", 'ours', 'being', 'at', 'having', \"needn't\", \"hasn't\", \"isn't\", 'wouldn', 'most', 'ma', 'mightn', 'while', 'your', 'too', 'other', 'themselves', 'than', 'was', 'because', \"shan't\", 'am', 'from', 'after', 'how', 'if', 'had', \"it's\", 'now', 'won', 'a', 'do', 'isn', 'why', 'these', 'during', 'between', 'on', 'only', 'of', \"you're\", 'haven', 'his', 'for', 'off', 'ain', 'be', 'such', \"you'd\", 'shan', 'what', 'm', \"mustn't\", 'no', 'there', 'that', 'all', 'd', 'just', 'its', 'over', 'in', 'needn', 'hadn', 'up', 'it', 'has', 'an', 've', \"hadn't\", 'ourselves', 'does', 'couldn', 'same', \"doesn't\", 'my', 'some', \"won't\", 'through', \"shouldn't\", 'is', 'don', 'yours', 'weren', 'were', 'below', 'down', 'herself', 'been', 'nor', 'but', 'myself', 'should', 'shouldn', 'to', 'once', 'have', 'with', 'whom', \"she's\", 'out', 'each', 'about', 'did', \"you'll\", 'hello', 'can', 'o', \"wasn't\", \"mightn't\", 'they', 'wasn', \"couldn't\", 'doing', 'himself', 'theirs', 'any', 't', 'into', 'doesn', 'by', \"you've\", 'this', 'are', \"don't\", 'here', 'further', 'we', 's', 'itself', 'y', 'our', 'when', \"didn't\", 'under', 'where', 'as', 'more', 'yourselves', 'own', 'you', 'few', 'so', 'i', 'against', \"aren't\", \"should've\", 'yourself', 'hers', 'will', 'him', 'll', \"that'll\", \"haven't\"}\n"
     ]
    }
   ],
   "source": [
    "stopwords_set.add('hello')\n",
    "stopwords_set.remove('the')\n",
    "stopwords_set.remove('me')\n",
    "\n",
    "print('불용어 개수 :', len(stopwords_set))\n",
    "print('불용어 출력 :',stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7961aad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:05:44.321948Z",
     "start_time": "2023-12-27T05:05:44.308656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'myself', 'ourselves', 'ours', 'my', 'we', 'i', 'me', 'our'}\n"
     ]
    }
   ],
   "source": [
    "my_stopwords_set = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves'}\n",
    "\n",
    "print(my_stopwords_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b07bf",
   "metadata": {},
   "source": [
    "### 불용어 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2bf75bd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:06:05.636469Z",
     "start_time": "2023-12-27T05:06:05.627177Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words_set = set(stopwords.words('english'))\n",
    "\n",
    "# 불용어 제거\n",
    "cleaned_words = []\n",
    "\n",
    "for word in cleaned_by_freq_len:\n",
    "    if word not in stop_words_set:\n",
    "        cleaned_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1af30394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:06:11.326502Z",
     "start_time": "2023-12-27T05:06:11.317991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전: 169\n",
      "불용어 제거 후: 67\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거 결과 확인\n",
    "print('불용어 제거 전:', len(cleaned_by_freq_len))\n",
    "print('불용어 제거 후:', len(cleaned_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58091ca",
   "metadata": {},
   "source": [
    "### 불용어 처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc793433",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:06:38.574055Z",
     "start_time": "2023-12-27T05:06:38.557542Z"
    }
   },
   "outputs": [],
   "source": [
    "# 불용어 제거 함수\n",
    "def clean_by_stopwords(tokenized_words, stop_words_set):\n",
    "    cleaned_words = []\n",
    "    \n",
    "    for word in tokenized_words:\n",
    "        if word not in stop_words_set:\n",
    "            cleaned_words.append(word)\n",
    "            \n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e416f3b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:07:26.890347Z",
     "start_time": "2023-12-27T05:07:26.875560Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_words = clean_by_stopwords(tokenized_words, stop_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbd15c54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:07:43.763835Z",
     "start_time": "2023-12-27T05:07:43.752833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9fdc34b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:07:31.118498Z",
     "start_time": "2023-12-27T05:07:31.104530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e2b72",
   "metadata": {},
   "source": [
    "## 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c49c61",
   "metadata": {},
   "source": [
    "### 대소문자 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30a87640",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:16:34.133395Z",
     "start_time": "2023-12-27T05:16:34.115563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what can i do for you? do your homework now.\n"
     ]
    }
   ],
   "source": [
    "text = \"What can I do for you? Do your homework now.\"\n",
    "\n",
    "# 소문자로 변환\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74d3471",
   "metadata": {},
   "source": [
    "### 규칙 기반 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb5e06f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:16:55.543409Z",
     "start_time": "2023-12-27T05:16:55.527276Z"
    }
   },
   "outputs": [],
   "source": [
    "# 동의어 사전\n",
    "synonym_dict = {'US':'USA', 'U.S':'USA', 'Ummm':'Umm', 'Ummmm':'Umm' }\n",
    "text = \"She became a US citizen. Ummmm, I think, maybe and or.\"\n",
    "normalized_words = []\n",
    "\n",
    "# 단어 토큰화\n",
    "tokenized_words = nltk.word_tokenize(text)\n",
    "\n",
    "for word in tokenized_words:\n",
    "    # 동의어 사전에 있는 단어라면, value에 해당하는 값으로 변환\n",
    "    if word in synonym_dict.keys():\n",
    "        word = synonym_dict[word]\n",
    "\n",
    "    normalized_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ca372de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:17:32.441220Z",
     "start_time": "2023-12-27T05:17:32.433935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'became', 'a', 'USA', 'citizen', '.', 'Umm', ',', 'I', 'think', ',', 'maybe', 'and', 'or', '.']\n"
     ]
    }
   ],
   "source": [
    "print(normalized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6036dcc",
   "metadata": {},
   "source": [
    "## 어간 추출하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26db225",
   "metadata": {},
   "source": [
    "### 포터 스테머 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01bf2d21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:20:09.273276Z",
     "start_time": "2023-12-27T05:20:09.267261Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45bf8585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:20:15.584638Z",
     "start_time": "2023-12-27T05:20:15.572983Z"
    }
   },
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "text = \"You are so lovely. I am loving you now.\"\n",
    "porter_stemmed_words = []\n",
    "\n",
    "# 단어 토큰화\n",
    "tokenized_words = nltk.word_tokenize(text)\n",
    "\n",
    "# 포터 스테머의 어간 추출\n",
    "for word in tokenized_words:\n",
    "    stem = porter_stemmer.stem(word)\n",
    "    porter_stemmed_words.append(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "784b38d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:20:20.351613Z",
     "start_time": "2023-12-27T05:20:20.346179Z"
    }
   },
   "outputs": [],
   "source": [
    "# 단어 토큰화\n",
    "tokenized_words = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "abc7ace5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:20:40.280194Z",
     "start_time": "2023-12-27T05:20:40.271204Z"
    }
   },
   "outputs": [],
   "source": [
    "# 포터 스테머의 어간 추출\n",
    "for word in tokenized_words:\n",
    "    stem = porter_stemmer.stem(word)\n",
    "    porter_stemmed_words.append(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2021f9fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:20:47.011820Z",
     "start_time": "2023-12-27T05:20:46.998620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['You', 'are', 'so', 'lovely', '.', 'I', 'am', 'loving', 'you', 'now', '.']\n",
      "포터 스테머의 어간 추출 후: ['you', 'are', 'so', 'love', '.', 'i', 'am', 'love', 'you', 'now', '.', 'you', 'are', 'so', 'love', '.', 'i', 'am', 'love', 'you', 'now', '.']\n"
     ]
    }
   ],
   "source": [
    "print('어간 추출 전 :', tokenized_words)\n",
    "print('포터 스테머의 어간 추출 후:', porter_stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6587f79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:21:53.872523Z",
     "start_time": "2023-12-27T05:21:53.862000Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# 포터 스테머 어간 추출 함수\n",
    "def stemming_by_porter(tokenized_words):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    porter_stemmed_words = []\n",
    "\n",
    "    for word in tokenized_words:\n",
    "        stem = porter_stemmer.stem(word)\n",
    "        porter_stemmed_words.append(stem)\n",
    "\n",
    "    return porter_stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee570f0",
   "metadata": {},
   "source": [
    "### 랭커스터 스테머 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08c9ceb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T05:21:55.056697Z",
     "start_time": "2023-12-27T05:21:55.042086Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "text = \"You are so lovely. I am loving you now.\"\n",
    "lancaster_stemmed_words = []\n",
    "\n",
    "# 랭커스터 스테머의 어간 추출\n",
    "for word in tokenized_words:\n",
    "    stem = lancaster_stemmer.stem(word)\n",
    "    lancaster_stemmed_words.append(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f62e2b",
   "metadata": {},
   "source": [
    "# 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66505c4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:01:10.897693Z",
     "start_time": "2023-12-27T06:01:08.358520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "114ab95f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:02:13.578291Z",
     "start_time": "2023-12-27T06:02:13.558249Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/imdb.tsv\", delimiter=\"\\\\t\", engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17fd1e",
   "metadata": {},
   "source": [
    "## 대소문자 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "793208e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:02:47.161899Z",
     "start_time": "2023-12-27T06:02:47.152808Z"
    }
   },
   "outputs": [],
   "source": [
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4cf583",
   "metadata": {},
   "source": [
    "## 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "54570837",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:03:17.319902Z",
     "start_time": "2023-12-27T06:03:17.273704Z"
    }
   },
   "outputs": [],
   "source": [
    "df['word_tokens'] = df['review'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6ca5e8e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:03:43.900852Z",
     "start_time": "2023-12-27T06:03:43.690371Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f292cbf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:24:42.178194Z",
     "start_time": "2023-12-27T06:24:42.014807Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a82d1655",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:26:29.751555Z",
     "start_time": "2023-12-27T06:26:29.572664Z"
    }
   },
   "outputs": [],
   "source": [
    "from preprocess import clean_by_freq\n",
    "from preprocess import clean_by_len\n",
    "from preprocess import clean_by_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7aacf7cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:26:30.741547Z",
     "start_time": "2023-12-27T06:26:30.558049Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "df['cleaned_tokens'] = df['word_tokens'].apply(lambda x: clean_by_freq(x, 1))\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d49775a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:27:04.427642Z",
     "start_time": "2023-12-27T06:27:04.275015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'film', 'said', 'really', 'bad', 'movie', 'like', 'said', 'really', 'bad', 'movie', 'bad', 'one', 'film', 'like']\n"
     ]
    }
   ],
   "source": [
    "print(df['cleaned_tokens'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823b5e48",
   "metadata": {},
   "source": [
    "## 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2a909b0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:27:35.805144Z",
     "start_time": "2023-12-27T06:27:35.633157Z"
    }
   },
   "outputs": [],
   "source": [
    "from preprocess import stemming_by_porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "735f6469",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:27:41.827864Z",
     "start_time": "2023-12-27T06:27:41.661537Z"
    }
   },
   "outputs": [],
   "source": [
    "df['stemmed_tokens'] = df['cleaned_tokens'].apply(stemming_by_porter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cf12ea2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:27:45.798051Z",
     "start_time": "2023-12-27T06:27:45.651961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'film', 'said', 'realli', 'bad', 'movi', 'like', 'said', 'realli', 'bad', 'movi', 'bad', 'one', 'film', 'like']\n"
     ]
    }
   ],
   "source": [
    "print(df['stemmed_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d7c57fda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:32:10.752610Z",
     "start_time": "2023-12-27T06:32:10.553746Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"watching time chasers, it obvious that it was...</td>\n",
       "      <td>[``, watching, time, chasers, ,, it, obvious, ...</td>\n",
       "      <td>[one, film, said, really, bad, movie, like, sa...</td>\n",
       "      <td>[one, film, said, realli, bad, movi, like, sai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[i, saw, this, film, about, 20, years, ago, an...</td>\n",
       "      <td>[film, film]</td>\n",
       "      <td>[film, film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>[minor, spoilers, in, new, york, ,, joan, barn...</td>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, bar...</td>\n",
       "      <td>[new, york, joan, barnard, elvir, audrey, barn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>[i, went, to, see, this, film, with, a, great,...</td>\n",
       "      <td>[went, film, film, went, jump, send, n't, jump...</td>\n",
       "      <td>[went, film, film, went, jump, send, n't, jump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"yes, i agree with everyone on this site this ...</td>\n",
       "      <td>[``, yes, ,, i, agree, with, everyone, on, thi...</td>\n",
       "      <td>[site, movie, bad, even, movie, made, movie, s...</td>\n",
       "      <td>[site, movi, bad, even, movi, made, movi, spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"jennifer ehle was sparkling in \\\"\"pride and p...</td>\n",
       "      <td>[``, jennifer, ehle, was, sparkling, in, \\, ''...</td>\n",
       "      <td>[ehle, northam, wonderful, wonderful, ehle, no...</td>\n",
       "      <td>[ehl, northam, wonder, wonder, ehl, northam, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>[amy, poehler, is, a, terrific, comedian, on, ...</td>\n",
       "      <td>[role, movie, n't, author, book, author, autho...</td>\n",
       "      <td>[role, movi, n't, author, book, author, author...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"a plane carrying employees of a large biotech...</td>\n",
       "      <td>[``, a, plane, carrying, employees, of, a, lar...</td>\n",
       "      <td>[plane, ceo, search, rescue, mission, ceo, har...</td>\n",
       "      <td>[plane, ceo, search, rescu, mission, ceo, harl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>[a, well, made, ,, gritty, science, fiction, m...</td>\n",
       "      <td>[gritty, movie, sci-fi, good, suspense, movie,...</td>\n",
       "      <td>[gritti, movi, sci-fi, good, suspens, movi, sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"incredibly dumb and utterly predictable story...</td>\n",
       "      <td>[``, incredibly, dumb, and, utterly, predictab...</td>\n",
       "      <td>[girl, girl]</td>\n",
       "      <td>[girl, girl]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  \"watching time chasers, it obvious that it was...   \n",
       "1  i saw this film about 20 years ago and remembe...   \n",
       "2  minor spoilers in new york, joan barnard (elvi...   \n",
       "3  i went to see this film with a great deal of e...   \n",
       "4  \"yes, i agree with everyone on this site this ...   \n",
       "5  \"jennifer ehle was sparkling in \\\"\"pride and p...   \n",
       "6  amy poehler is a terrific comedian on saturday...   \n",
       "7  \"a plane carrying employees of a large biotech...   \n",
       "8  a well made, gritty science fiction movie, it ...   \n",
       "9  \"incredibly dumb and utterly predictable story...   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  [``, watching, time, chasers, ,, it, obvious, ...   \n",
       "1  [i, saw, this, film, about, 20, years, ago, an...   \n",
       "2  [minor, spoilers, in, new, york, ,, joan, barn...   \n",
       "3  [i, went, to, see, this, film, with, a, great,...   \n",
       "4  [``, yes, ,, i, agree, with, everyone, on, thi...   \n",
       "5  [``, jennifer, ehle, was, sparkling, in, \\, ''...   \n",
       "6  [amy, poehler, is, a, terrific, comedian, on, ...   \n",
       "7  [``, a, plane, carrying, employees, of, a, lar...   \n",
       "8  [a, well, made, ,, gritty, science, fiction, m...   \n",
       "9  [``, incredibly, dumb, and, utterly, predictab...   \n",
       "\n",
       "                                      cleaned_tokens  \\\n",
       "0  [one, film, said, really, bad, movie, like, sa...   \n",
       "1                                       [film, film]   \n",
       "2  [new, york, joan, barnard, elvire, audrey, bar...   \n",
       "3  [went, film, film, went, jump, send, n't, jump...   \n",
       "4  [site, movie, bad, even, movie, made, movie, s...   \n",
       "5  [ehle, northam, wonderful, wonderful, ehle, no...   \n",
       "6  [role, movie, n't, author, book, author, autho...   \n",
       "7  [plane, ceo, search, rescue, mission, ceo, har...   \n",
       "8  [gritty, movie, sci-fi, good, suspense, movie,...   \n",
       "9                                       [girl, girl]   \n",
       "\n",
       "                                      stemmed_tokens  \n",
       "0  [one, film, said, realli, bad, movi, like, sai...  \n",
       "1                                       [film, film]  \n",
       "2  [new, york, joan, barnard, elvir, audrey, barn...  \n",
       "3  [went, film, film, went, jump, send, n't, jump...  \n",
       "4  [site, movi, bad, even, movi, made, movi, spec...  \n",
       "5  [ehl, northam, wonder, wonder, ehl, northam, l...  \n",
       "6  [role, movi, n't, author, book, author, author...  \n",
       "7  [plane, ceo, search, rescu, mission, ceo, harl...  \n",
       "8  [gritti, movi, sci-fi, good, suspens, movi, sc...  \n",
       "9                                       [girl, girl]  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
